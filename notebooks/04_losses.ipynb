{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558e238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961c797",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "Loss functions provide a <mark>quantitative measure of the current performance of a neural network</mark>. There are many to choose from, but the <mark>most appropriate will often depend on your task and the form of the targets and outputs</mark>. Similar to layers in a neural network, <mark>losses are either offered as classes (inheriting from `nn.Module`) or as functions in `nn.functional`.</mark>\n",
    "\n",
    "PyTorch provides implementations for many common losses (https://pytorch.org/docs/stable/nn.html#loss-functions), and more advanced ones can be written by the user.\n",
    "\n",
    "In general, <mark>PyTorch losses will:</mark>\n",
    "- <mark>Take an `input` argument of predictions and a `target` argument of true values</mark>. In general, the first dimension is expected to be a batch dimension.\n",
    "- <mark>Have a *reduction* method, which determines how the final value is produced</mark>. The loss of each item in the batch will first be computed in isolation, then <mark>these can either be returned as an (N,) tensor (`reduction='none'`), or they can be reduced to the mean (`reduction='mean'` default) or the sum (`reduction='sum'`).</mark>\n",
    "\n",
    "The <mark>losses in PyTorch make strong assumptions on the inputs and targets (shapes, normalisation, log-space, logits, etc.), and often this isn't indicated in the name</mark>, so it is best check the docs to see what exactly is expected.\n",
    "\n",
    "Additionally, <mark>most losses have a `weight`, the effect of which varies between loss function and doesn't always behave as expected (to a HEP person). Additionally, they must be provided during initialisation</mark>, rather than vary per batch. <mark>If decent weight handling is required, write your own inheriting losses, or see mine: https://github.com/GilesStrong/lumin/blob/master/lumin/nn/losses/basic_weighted.py</mark>\n",
    "\n",
    "Below will be a few common losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4ecdb",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "For classification tasks with only two classes, the DNN can have a single output with a sigmoid output activation. The <mark>binary cross entropy function can then be used to quantify performance.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a291e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,1)  # pre-activation values of the DNN output, for a batch size of 10\n",
    "targs = torch.randint(0,2, size=(10,1)).float()  # random binary targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497a002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34eb6cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7286)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.sigmoid(logit), targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc038b",
   "metadata": {},
   "source": [
    "This is the mean binary cross-entropy for our batch. We could instead get the <mark>raw BCE per element:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3136ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd9740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9069],\n",
       "        [0.8259],\n",
       "        [0.3929],\n",
       "        [0.6429],\n",
       "        [1.1332],\n",
       "        [0.4033],\n",
       "        [0.8378],\n",
       "        [1.1399],\n",
       "        [0.3644],\n",
       "        [0.6391]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.sigmoid(logit), targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc1a7",
   "metadata": {},
   "source": [
    "In the above, we took the logits and applied a sigmoid activation to them, which involves taking the exponential of the logits. The BCE then compute the natural log of the predictions. <mark>One can save time and numerical precision, by instead computing the BCE directly from the logits:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b17d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57bfef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7286)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba62a4",
   "metadata": {},
   "source": [
    "### Multi-label classification\n",
    "This is similar to binary classification, except now we are predicting which <mark>non-mutually-exclusive Boolean properties the inputs have. Again we can use sigmoids for each of the targets, and BCE for the loss.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db75611",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,5)  # pre-activation values of the DNN output, for a batch size of 10 for 5 labels\n",
    "targs = torch.randint(0,2, size=(10,5)).float()  # random binary targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09933ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f17258ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7243)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.sigmoid(logit), targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e419b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be1e7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5836, 1.1112, 0.7216, 0.7140, 1.0407],\n",
       "        [1.1576, 0.3898, 0.5819, 0.6702, 1.1884],\n",
       "        [0.6334, 1.3086, 0.4543, 0.5928, 0.4659],\n",
       "        [0.8954, 0.5466, 0.6364, 1.2228, 0.8511],\n",
       "        [0.4344, 0.3651, 1.1878, 0.3831, 0.7508],\n",
       "        [0.7620, 0.6174, 0.5119, 0.8727, 0.4796],\n",
       "        [1.0074, 0.4904, 1.0814, 1.1764, 1.1757],\n",
       "        [0.6722, 0.4820, 1.1738, 0.3276, 1.0157],\n",
       "        [0.3354, 0.4324, 0.4923, 0.6751, 0.5781],\n",
       "        [0.3370, 0.4880, 1.0552, 0.5384, 0.5494]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(torch.sigmoid(logit), targs)\n",
    "loss  # reduction none, now gives the BCE per lable per item in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8420aac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8342],\n",
       "        [0.7976],\n",
       "        [0.6910],\n",
       "        [0.8305],\n",
       "        [0.6243],\n",
       "        [0.6487],\n",
       "        [0.9862],\n",
       "        [0.7343],\n",
       "        [0.5027],\n",
       "        [0.5936]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean(-1, keepdim=True)  # we can get the mean loss per item ourselves, though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8c156",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "Extending binary classification to the case where items belong to <mark>one and only one class, and there are more than two classes. The loss here is the categorical cross-entropy, which works by comparing the predicted probabilities that an item belongs to each of the classes to the true class it belongs to</mark>. This requires that <mark>per item, the logits are normalised to one: the softmax activation will perform this normalisation. **However** none of the pyTorch CCE losses actually expect a softmaxed input...</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d65dc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,5)  # pre-activation values of the DNN output, for a batch size of 10 for 5 classes\n",
    "targs = torch.randint(0,5, size=(10,))  # random targets for five classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce655bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1bd5755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5307)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)  # Unlike BCELoss, the CrossEntropyLoss expects the logits. Really this should be called CrossEntropyWithLogitsLoss, but hey ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10154345",
   "metadata": {},
   "source": [
    "Alternative, <mark>if you do want to have a softmax output, there is the negative log likelihood loss, which expects... the log of the softmaxed outputs.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba701c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af7c378a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5307)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(F.softmax(logit, dim=-1).log(), targs)  # the dim=-1 indicates to normalise over the last dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6446d",
   "metadata": {},
   "source": [
    "<mark>Alternatively, we can use the logsoftmax activation function:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46c41274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5307)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(F.log_softmax(logit, dim=-1), targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c4a60",
   "metadata": {},
   "source": [
    "#### Multi-d multi-class classification\n",
    "If predicting the class of 2D data, or higher, the expected tensor shape for:\n",
    " - inputs is (batch, class, x, y,...)\n",
    " - targets is (batch, x, y,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae286ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,5,2,3,4)  # pre-activation values of the DNN output, for a batch size of 10 for 5 classes over a cuboid\n",
    "targs = torch.randint(0,5, size=(10,2,3,4))  # random targets for five classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85459b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cbdfbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6519)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)  # Unlike BCELoss, the CrossEntropyLoss expects the logits. Really this should be called CrossEntropyWithLogitsLoss, but hey ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38a1a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a27030",
   "metadata": {},
   "source": [
    "<mark>Remember to normalise over the class dimension!!</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b5ab8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6519)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(F.softmax(logit, dim=1).log(), targs)  # remember to normalise over the class dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba284872",
   "metadata": {},
   "source": [
    "### Regression\n",
    "Regression problems involve predicting float targets. <mark>Typically no output activation is used, such that outputs linear map to [-inf,inf]. In such problems, the loss should scale with the error on the prediction.</mark> Common choices are:\n",
    "- squared error (p-t)**2\n",
    "- absolute error |p-t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d126bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,1)  # Outputs of the DNN output\n",
    "targs = torch.rand(10,1)  # random targets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eeae1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()  # Mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef1a54b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0881)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "857becc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()  # L1 loss is the absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc280743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2216)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b986d6",
   "metadata": {},
   "source": [
    "## Functional losses\n",
    "As mentioned, <mark>function versions of the losses exist, too, e.g.:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a409036",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,1)  # Outputs of the DNN output\n",
    "targs = torch.rand(10,1)  # random targets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cee51ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1216)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(logit, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40da17",
   "metadata": {},
   "source": [
    "## Custom loss function\n",
    "<mark>Class-based losses inherit from `nn.Module` so making our own is quite easy. We can even inherit from existing losses that are close to what we want.</mark>\n",
    "Let's make a loss that takes the squared-error on predictions and then divides it by the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7170f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalMSE(nn.MSELoss):  # Inherit from the basic MSELoss\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction='none')  # Set the reduction to none such that the SE shape matches the targets\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        se = super().forward(input, target)  # Compute the MSE \n",
    "        fse = se/target\n",
    "        return torch.mean(fse)  # return the mean fractional squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5aa8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,2)  # Outputs of the DNN output\n",
    "targs = torch.rand(10,2)  # random targets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17e01d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = FractionalMSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e161676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2730)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
