{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c92a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319680d8",
   "metadata": {},
   "source": [
    "## Differentiating tensors\n",
    "In part 01 we looked at how tensors can be used to store data and perform vectorised operations.\n",
    "In PyTorch it is possible to differentiate the results of such operations with respect to the input variables, if desired.\n",
    "\n",
    "To do this, the <mark>dependent variable (`Tensor`) must be initialised with `requires_grad` set to `True`</mark>. Then, anytime the tensor is used in an operation <mark>the gradient of that function will be included in the result, eventually allowing it to be differentiated</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce9b20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_var = torch.tensor([6.0], requires_grad=True)\n",
    "dep_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0797780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1603])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f1877d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9617], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = data*dep_var\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7de891",
   "metadata": {},
   "source": [
    "Note that the <mark>value has a `grad_fn`, which means it can be differentiated.</mark>\n",
    "\n",
    "<mark>`torch.autograd` contains various functions to help with this, e.g. `torch.autograd.grad`:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66a3352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cced16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1603]),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(outputs=value, inputs=dep_var) # differentiation of the value dep_var · data with respect to dep_var ( = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f91bf",
   "metadata": {},
   "source": [
    "### Differentiating functions\n",
    "Above, we performed a function on a tensor and got the result, which we then differentiated with respect to the dependent input. <mark>It is also possible to differentiate functions directly for given inputs, without getting the results of the functions. I don't particularly like this.</mark>\n",
    "\n",
    "Note that it also computes the gradient with respect to the data, even though the data wasn't set to require gradient.\n",
    "\n",
    "These methods are <mark>mainly designed for *functional* programming, but you'd be better off looking into the [functorch ](https://pytorch.org/functorch/stable/) extension for PyTorch, or just using [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e14a7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data,dep_var):\n",
    "    return data*dep_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b8ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6.]]), tensor([[0.1603]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(func, (data,dep_var)) # derivative with respect to (data, dep_var) which gives (dep_var, data) of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d55355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[0.]]), tensor([[1.]])), (tensor([[1.]]), tensor([[0.]])))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.hessian(func, (data,dep_var)) # tensor of second derivatives ([[∂/∂data ∂data, ∂/∂data ∂dep_var], [∂/∂dep_var ∂data, ∂/∂dep_var ∂dep_var]]), which gives the antidiagonal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d454f9",
   "metadata": {},
   "source": [
    "### Multiple variables and second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6e54d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var_0 = torch.tensor([6.0], requires_grad=True)\n",
    "dep_var_1 = torch.tensor([-2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8194eee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4519])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45d71872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0742], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = (dep_var_0**data)+dep_var_1.square()\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd7bbc",
   "metadata": {},
   "source": [
    "<mark>To differentiate w.r.t. multiple variables, include them as a tuple in the inputs</mark>. The <mark>`retain_graph` argument allows us to to recompute the gradient, if necessary, and the `create_graph` argument makes the output also have a `grad_fn`, is applicable</mark>, which allows to compute the second derivative, eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78d8346e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0179], grad_fn=<WhereBackward0>),\n",
       " tensor([-4.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac = grad(outputs=value, inputs=(dep_var_0, dep_var_1), retain_graph=0, create_graph=True)\n",
    "jac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379409d",
   "metadata": {},
   "source": [
    "Since the gradient has a <mark>`grad_fn`, we can compute the second derivative, too. Note that we don't get the full Hessian matrix, though, only the diagonal.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3bc9b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0073], grad_fn=<WhereBackward0>),\n",
       " tensor([2.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(outputs=jac, inputs=(dep_var_0, dep_var_1), retain_graph=True, create_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8888e",
   "metadata": {},
   "source": [
    "#### Full Hessian\n",
    "\n",
    "If you know that you'll be later computing the Hessian, or even just Jacobians, I find it best to have all the dependent variables in a single Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "270159c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_vars = torch.tensor([6.0, -2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9f91b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0742], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = (dep_vars[0]**data)+dep_vars[1].square()\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286249b",
   "metadata": {},
   "source": [
    "We compute the Jacobian as normal. This now returns the Jacobian in a single tensor, rather than a tuple of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b596f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0179, -4.0000], grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac = grad(outputs=value, inputs=dep_vars, retain_graph=True, create_graph=True)\n",
    "jac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372b58a",
   "metadata": {},
   "source": [
    "Now we try to compute the Hessian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a58fc079",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grad(outputs\u001b[39m=\u001b[39;49mjac, inputs\u001b[39m=\u001b[39;49mdep_vars, retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:285\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    280\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 285\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(t_outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "grad(outputs=jac, inputs=dep_vars, retain_graph=True, create_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952b922",
   "metadata": {},
   "source": [
    "Oh dear! The output needs to be a single value, and we provided two values. Instead we can supply each value in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a81e19fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.0073, 0.0000], grad_fn=<AddBackward0>),),\n",
       " (tensor([0., 2.], grad_fn=<AddBackward0>),))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(outputs=jac[0][0], inputs=dep_vars, retain_graph=True, create_graph=True), grad(outputs=jac[0][1], inputs=dep_vars, retain_graph=True, create_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119d783",
   "metadata": {},
   "source": [
    "<mark>So now we get the full Hessian matrix. (In this case the off-diagonals were zero, but they might not always be)</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30a926",
   "metadata": {},
   "source": [
    "## Batched gradients and better Hessians\n",
    "We already saw that the <mark>`grad` function has trouble dealing with non-scalar outputs, which meant we needed to call it twice and get a tuple</mark>. A more general way to do this, would be to <mark>iterate over each element of the Jacobian and stack the Hessian rows into a tensor</mark>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "326a736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0073, 0.0000],\n",
       "        [0.0000, 2.0000]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([grad(outputs=j, inputs=dep_vars, retain_graph=True, create_graph=True)[0] for j in jac[0].unbind(0)])  # unbind alows us to iterate through the tensor along the specified dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce225e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 µs ± 1.32 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.stack([grad(outputs=j, inputs=dep_vars, retain_graph=True, create_graph=True)[0] for j in jac[0].unbind(0)])  # unbind allows us to iterate through the tensor along the specified dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15c138",
   "metadata": {},
   "source": [
    "A slightly <mark>quicker way is to still feed in the full Jacobian, but use the `grad_outputs` to *switch on* which element we want to differentiate. `torch.eye` creates an identity matrix of a given size, and iterating through it will provide one-hot vectors.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a59e66d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(len(jac[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baccbcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0073, 0.0000],\n",
       "        [0.0000, 2.0000]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([grad(outputs=jac[0], inputs=dep_vars, grad_outputs=i, retain_graph=True, create_graph=True)[0] for i in torch.eye(len(jac[0])).unbind(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df7f4931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 µs ± 6.75 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "torch.stack([grad(outputs=jac[0], inputs=dep_vars, grad_outputs=i, retain_graph=True, create_graph=True)[0] for i in torch.eye(len(jac[0])).unbind(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46b4ae",
   "metadata": {},
   "source": [
    "### Vectorised method\n",
    "The above method works, but still relies on a python for-loop to provide serial calls to `grad`. It would be better to instead <mark>perform all calls in parallel. We can do this using the PyTorch vmap function, although it is still experimental. It is only slightly faster.</mark>\n",
    "\n",
    "`vmap` takes a function and a set of input arguments and will implicitly compute the function values by unbinding the inputs, and will then stack the results to a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5daa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._vmap_internals import _vmap as vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "851c7d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0073, 0.0000],\n",
       "        [0.0000, 2.0000]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(lambda i: grad(outputs=jac[0], inputs=dep_vars, grad_outputs=i, retain_graph=True, create_graph=True)[0])(torch.eye(len(jac[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4aee8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258 µs ± 6.32 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "vmap(lambda i: grad(outputs=jac[0], inputs=dep_vars, grad_outputs=i, retain_graph=True, create_graph=True)[0])(torch.eye(len(jac[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7ba6b",
   "metadata": {},
   "source": [
    "### Batched gradients\n",
    "The <mark>\"non-scalar output\" issue doesn't just apply to Hessians</mark>, what if we want to efficiently differentiate a batch of items independently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c45357f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_vars = torch.tensor([6.0, -2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22bd7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4aa04af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.3601, 7.9987],\n",
       "        [5.2061, 5.3302],\n",
       "        [5.7386, 5.1194],\n",
       "        [7.3961, 8.4607],\n",
       "        [8.3072, 7.9371],\n",
       "        [9.2684, 8.0694],\n",
       "        [5.5813, 5.3362],\n",
       "        [5.4156, 7.7234],\n",
       "        [7.7857, 7.1862],\n",
       "        [5.4934, 7.4062]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = (dep_vars[0]**data)+dep_vars[1].square()\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2494440f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grad(outputs\u001b[39m=\u001b[39;49mvalues, inputs\u001b[39m=\u001b[39;49mdep_vars, retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:285\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    280\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 285\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(t_outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "grad(outputs=values, inputs=dep_vars, retain_graph=True, create_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ea09a",
   "metadata": {},
   "source": [
    "The <mark>trick is to reuse our vmap'd Jacobian, however even if we iterate over each row of the output values, it still isn't a scalar value; we need iterate over each element</mark>.\n",
    "\n",
    "Rather than having the iteration adapt to every possible output shape, <mark>it is instead more convenient to write a generalised Jacobian function that works for any shape, by flattening and reshaping the inputs</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "571ea147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(y: Tensor, x: Tensor, create_graph: bool = False, allow_unused: bool = True) -> Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the Jacobian (dy/dx) of y with respect to variables x. x and y can have multiple elements.\n",
    "    If y has multiple elements then computation is vectorised via vmap.\n",
    "\n",
    "    Arguments:\n",
    "        y: tensor to be differentiated\n",
    "        x: dependent variables\n",
    "        create_graph: If True, graph of the derivative will\n",
    "            be constructed, allowing to compute higher order derivative products.\n",
    "            Default: False.\n",
    "        allow_unused: If False, specifying inputs that were not\n",
    "            used when computing outputs (and therefore their grad is always\n",
    "\n",
    "    Returns:\n",
    "        dy/dx tensor of shape y.shape+x.shape\n",
    "    \"\"\"\n",
    "\n",
    "    if len(y) == 0:\n",
    "        return None\n",
    "    flat_y = y.reshape(-1)\n",
    "\n",
    "    def get_vjp(v: Tensor) -> Tensor:\n",
    "        return torch.autograd.grad(flat_y, x, v, retain_graph=True, create_graph=create_graph, allow_unused=allow_unused)[0].reshape(x.shape)\n",
    "\n",
    "    return vmap(get_vjp)(torch.eye(len(flat_y), device=y.device)).reshape(y.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "750f335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0389, -4.0000],\n",
       "         [ 0.5155, -4.0000]],\n",
       "\n",
       "        [[ 0.0210, -4.0000],\n",
       "         [ 0.0353, -4.0000]],\n",
       "\n",
       "        [[ 0.0894, -4.0000],\n",
       "         [ 0.0117, -4.0000]],\n",
       "\n",
       "        [[ 0.3862, -4.0000],\n",
       "         [ 0.6205, -4.0000]],\n",
       "\n",
       "        [[ 0.5851, -4.0000],\n",
       "         [ 0.5019, -4.0000]],\n",
       "\n",
       "        [[ 0.8143, -4.0000],\n",
       "         [ 0.5313, -4.0000]],\n",
       "\n",
       "        [[ 0.0674, -4.0000],\n",
       "         [ 0.0360, -4.0000]],\n",
       "\n",
       "        [[ 0.0458, -4.0000],\n",
       "         [ 0.4553, -4.0000]],\n",
       "\n",
       "        [[ 0.4688, -4.0000],\n",
       "         [ 0.3434, -4.0000]],\n",
       "\n",
       "        [[ 0.0557, -4.0000],\n",
       "         [ 0.3883, -4.0000]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(values, dep_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31479af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(values, dep_vars).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6c1ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape, dep_vars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084a908",
   "metadata": {},
   "source": [
    "This gives us the Jacobian of every element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347677c2",
   "metadata": {},
   "source": [
    "## No-grad contexts\n",
    "<mark>Calculations performed involving tensors with gradient incur an increased cost in terms of time and memory</mark>. In cases where gradient tracking isn't required, the <mark>context manager `no_grad` may be used:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75b1d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_vars = torch.tensor([6.0, -2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c38ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86c38851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.6 µs ± 584 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(dep_vars[0]**data)+dep_vars[1].square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e678e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9 µs ± 188 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    (dep_vars[0]**data)+dep_vars[1].square()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da929ae0",
   "metadata": {},
   "source": [
    "The <mark>`inference_mode` context is even more performant:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff0d4149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8 µs ± 591 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.inference_mode():\n",
    "    (dep_vars[0]**data)+dep_vars[1].square()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25e0a0",
   "metadata": {},
   "source": [
    "## Modifying tensor with gradient\n",
    "<mark>Once a tensor is set to have gradient, in-place modifications to it will result in an exception.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be5123c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7cda3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54c45066",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "190f9519",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "a[0] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fa41f",
   "metadata": {},
   "source": [
    "Here, the <mark>`no_grad` context can be used</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2fc3744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a[0] = 1.0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc01bf",
   "metadata": {},
   "source": [
    "Or its `data` can be modified directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71dcefeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.], requires_grad=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data[0] = 7.0\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
