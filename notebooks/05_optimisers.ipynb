{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f473a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280464d0",
   "metadata": {},
   "source": [
    "## Optimisers\n",
    "Optimisers are <mark>classes responsible for updating parameters according to their effect on the loss.\n",
    "The general update rule is:</mark>\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\gamma\\nabla_{\\theta_t}\\!\\mathcal{L}$$\n",
    "\n",
    "The gradient on parameter $\\theta$, $\\nabla_{\\theta_t}\\!\\mathcal{L}$ can be computed by backpropagating the loss value to the parameter. <mark>The learning rate $\\gamma$ controls how much the parameter is changed by the gradient</mark>; how large a step size the optimiser makes. The fact that the change is negative, means that the loss should be such that minimising it results in better performance.\n",
    "\n",
    "The <mark>loss is often evaluated using a batch of data points, rather than all data, or just one point. The result is a trade-off between speed and precise evaluation of the loss.</mark> The stochastic nature also reduces the chance that the DNN overfits to the training data.\n",
    "\n",
    "PyTorch implements a variety of optimisers (https://pytorch.org/docs/stable/optim.html). See https://ruder.io/optimizing-gradient-descent/index.html for a good overview.\n",
    "\n",
    "We'll stick with the <mark>standard SGD for now. When instantiating an `Optimizer` the parameters to be optimised must be provided, along with the necessary hyper-parameters of the optimisation algorithm.</mark> We'll starts with a very simple set of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a5b0bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.2790, -0.5656, -0.0903],\n",
       "                      [ 0.0068, -0.2715, -0.2693],\n",
       "                      [-0.3594, -0.3630,  0.4677],\n",
       "                      [ 0.2626,  0.3507,  0.4352],\n",
       "                      [ 0.1235, -0.5480,  0.3981],\n",
       "                      [ 0.3639, -0.2865, -0.2395],\n",
       "                      [-0.1944,  0.1937, -0.4398],\n",
       "                      [ 0.1796, -0.3070,  0.2573],\n",
       "                      [ 0.2668,  0.3852, -0.5520],\n",
       "                      [-0.5128, -0.3175,  0.5120]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.2366, -0.4597, -0.1735, -0.4391, -0.4639,  0.1803, -0.5023,  0.0026,\n",
       "                       0.0280, -0.3805])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.1051, -0.2527, -0.2221,  0.1301, -0.2620, -0.1702, -0.1226, -0.2376,\n",
       "                       -0.1948, -0.2048]])),\n",
       "             ('2.bias', tensor([-0.2743]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(3,10), nn.ReLU(), nn.Linear(10,1), nn.Sigmoid())\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fcf64",
   "metadata": {},
   "source": [
    "<mark>To optimise the parameters of the model, we pass its `.paramaters()` generator to the optimiser constructor, which allows it to always be able to access the parameters.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e0e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(params=model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76255f",
   "metadata": {},
   "source": [
    "We also need a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab484e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b825d",
   "metadata": {},
   "source": [
    "Now we pass some data through the network to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9698f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(20,3)\n",
    "inputs[10:] += 0.25\n",
    "targets = torch.zeros(20,1)\n",
    "targets[10:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef154de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3746],\n",
       "        [0.4166],\n",
       "        [0.3722],\n",
       "        [0.3707],\n",
       "        [0.4085],\n",
       "        [0.3947],\n",
       "        [0.4036],\n",
       "        [0.3702],\n",
       "        [0.3399],\n",
       "        [0.4305],\n",
       "        [0.4044],\n",
       "        [0.3465],\n",
       "        [0.3991],\n",
       "        [0.4298],\n",
       "        [0.3400],\n",
       "        [0.3747],\n",
       "        [0.3438],\n",
       "        [0.4319],\n",
       "        [0.4158],\n",
       "        [0.4281]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e207902",
   "metadata": {},
   "source": [
    "Now we compute the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd365fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7172, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe8242",
   "metadata": {},
   "source": [
    "At this point we want to <mark>ensure that the parameters do not have any gradient value, e.g. left over from previous updates</mark>. In this case, <mark>we can see that the `.grad` attributes are `None`.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6d61995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.2790, -0.5656, -0.0903],\n",
       "         [ 0.0068, -0.2715, -0.2693],\n",
       "         [-0.3594, -0.3630,  0.4677],\n",
       "         [ 0.2626,  0.3507,  0.4352],\n",
       "         [ 0.1235, -0.5480,  0.3981],\n",
       "         [ 0.3639, -0.2865, -0.2395],\n",
       "         [-0.1944,  0.1937, -0.4398],\n",
       "         [ 0.1796, -0.3070,  0.2573],\n",
       "         [ 0.2668,  0.3852, -0.5520],\n",
       "         [-0.5128, -0.3175,  0.5120]], requires_grad=True),\n",
       " None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight, model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3240c59",
   "metadata": {},
   "source": [
    "Just in case, though <mark>we will ensure that they are all zero or None.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a0644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f89ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231c735",
   "metadata": {},
   "source": [
    "Now we can backpropagate the gradient of the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed23941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded84195",
   "metadata": {},
   "source": [
    "Now when we check the gradients on the parameters, we'll see that they are non-zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6d07a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0099,  0.0027,  0.0070],\n",
       "        [-0.0032,  0.0056,  0.0026],\n",
       "        [-0.0019, -0.0014,  0.0178],\n",
       "        [-0.0060,  0.0013, -0.0129],\n",
       "        [ 0.0092,  0.0040,  0.0179],\n",
       "        [ 0.0316,  0.0210,  0.0195],\n",
       "        [ 0.0021, -0.0052,  0.0123],\n",
       "        [ 0.0039, -0.0026,  0.0119],\n",
       "        [ 0.0223,  0.0103,  0.0124],\n",
       "        [-0.0071, -0.0023,  0.0005]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd675265",
   "metadata": {},
   "source": [
    "<mark>The values of the parameters haven't changed, yet. We need to perform an update step with the optimiser</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2388dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "694bf64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2789, -0.5656, -0.0904],\n",
       "        [ 0.0069, -0.2715, -0.2693],\n",
       "        [-0.3594, -0.3629,  0.4675],\n",
       "        [ 0.2626,  0.3507,  0.4353],\n",
       "        [ 0.1234, -0.5480,  0.3979],\n",
       "        [ 0.3636, -0.2867, -0.2397],\n",
       "        [-0.1944,  0.1937, -0.4400],\n",
       "        [ 0.1795, -0.3069,  0.2572],\n",
       "        [ 0.2665,  0.3851, -0.5521],\n",
       "        [-0.5127, -0.3174,  0.5120]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59bbe1",
   "metadata": {},
   "source": [
    "<mark>The parameters have now updated slightly. They still have their gradients, though, which is why it is important that we always zero them before backpropagating the loss.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a63e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0099,  0.0027,  0.0070],\n",
       "        [-0.0032,  0.0056,  0.0026],\n",
       "        [-0.0019, -0.0014,  0.0178],\n",
       "        [-0.0060,  0.0013, -0.0129],\n",
       "        [ 0.0092,  0.0040,  0.0179],\n",
       "        [ 0.0316,  0.0210,  0.0195],\n",
       "        [ 0.0021, -0.0052,  0.0123],\n",
       "        [ 0.0039, -0.0026,  0.0119],\n",
       "        [ 0.0223,  0.0103,  0.0124],\n",
       "        [-0.0071, -0.0023,  0.0005]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
