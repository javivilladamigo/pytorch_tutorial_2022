{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1a810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dff7a5",
   "metadata": {},
   "source": [
    "## NN module\n",
    "The <mark>`NN` module contains classes, functions, and other modules for creating neural networks from smaller building blocks</mark>. The majority of classes inherit from `nn.Module`, which provides a lot of functionality for tracking learnable parameters and acting on inputted tensors.\n",
    "\n",
    "A class inheriting from `nn.Module` will have a:\n",
    "- `forward()` method, where it can act on incoming tensors. The `__call__` method will call the `forward` method, meaning that once instantiated, the object can be called, in order to call it's `forward` method, e.g. `my_module(x)` will pass `x` to `my_module.forward(x)` and return the output.\n",
    "- `parameters()` method, which is a provides a recursive generator that yields all `nn.Parameter`s stored by the `Module` and any other `nn.Module`s it stores\n",
    "- `state_dict()` method, which returns a dictionary of the current values of all `nn.Parameter`s and registered buffers stored by the `Module` and any other `nn.Module`s it stores\n",
    "- `to()` method, which will recursively place all other `nn.Module`s and `nn.Parameter`s stored by the `Module` onto the specified device\n",
    "\n",
    "\n",
    "### nn.Parameter\n",
    "<mark>`nn.Parameter`s are basically just `Tensor`s with `require_grad=True`</mark>, except that when they are declared as attributes of an `nn.Module`, they will be treated specially. E.g. they are returned by the `parameters()` generator, and stored in the `state_dict`. As we'll see later, <mark>optimisers in PyTorch are initialised using the `parameters()` generator, so `nn.Parameter`s will therefore be updated by gradient descent. Additionally, loading and saving of a `nn.Module` is done via its `state_dict`, so the values of `nn.Parameter`s will be loaded and saved, too.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1030a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # The super constructor must always be called, otherwise no parameters can be assigned\n",
    "        self.tensor_a = torch.tensor([3.], requires_grad=True)  # here we declare a tensor with gradient\n",
    "        self.param_b = nn.Parameter(torch.tensor([2.]))  # here we declare a parameter with gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5858b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = MyModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf272ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([2.], requires_grad=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.parameters())  # note that only param_b is listed, tensor_a is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66097f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('param_b', tensor([2.]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.state_dict()  # similarly, only param_b is listed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468e444",
   "metadata": {},
   "source": [
    "<mark>Parameters can also be included as `nn.ParameterList` and `nn.ParameterDict` classes, which act similarly to lists and dictionaries, except that they will also be identified as parameters of the module.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfdeb5",
   "metadata": {},
   "source": [
    "### Buffers\n",
    "Sometimes we have <mark>values that we want to keep constant during optimisation, but also want to be included in the `state_dict` such that they can be easily loaded and saved. Such values can be registered as *buffers*:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0dbbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.tensor_a = torch.tensor([3.], requires_grad=True)\n",
    "        self.param_b = nn.Parameter(torch.tensor([2.]))\n",
    "        self.register_buffer('buffer_c', value)  # register the buffer with a given name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13892cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = MyModule(Tensor([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520c1cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([2.], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.parameters())  # buffer_c isn't included as a parameter, tensor_a is again ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83a29e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('param_b', tensor([2.])), ('buffer_c', tensor([-1.]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.state_dict()  # but is included in the state dict specifying also its value, tensor_a is again ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d37a0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.buffer_c  # the buffer appears as an attribute with the name that was provided when it was registered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e200c22",
   "metadata": {},
   "source": [
    "## Common classes\n",
    "There are many different classes implemented in PyTorch. See https://pytorch.org/docs/stable/nn.html for the full list. Described below are a <mark>few common examples.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2c90e",
   "metadata": {},
   "source": [
    "### Linear layers\n",
    "\n",
    "A common class is `nn.Linear`, which implements the linear transform `w.x+b`, where `w` and `b` are learnable parameters. These can be used for the \"hidden\" layers in feed-forward DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edb43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = nn.Linear(in_features=4, out_features=6)  # the layer expects 4 features in and will output 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d389cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.1635,  0.1237, -0.4205,  0.2092],\n",
       "                      [-0.1941,  0.4541, -0.3560,  0.4676],\n",
       "                      [-0.4014, -0.1734, -0.0160,  0.1873],\n",
       "                      [ 0.1532, -0.2695,  0.0614,  0.0183],\n",
       "                      [ 0.3869,  0.1485, -0.0024,  0.0363],\n",
       "                      [-0.1718,  0.4823, -0.1703,  0.4349]])),\n",
       "             ('bias',\n",
       "              tensor([-0.3623, -0.2044,  0.4976, -0.2128,  0.4963,  0.4201]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.state_dict()  # it has a weight (6,4) and a bias (6), which are initialised at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d69e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 6]), <AddmmBackward0 at 0x1179c1db0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,4)\n",
    "x = lin(x)  # this calls the forward method of the linear layer, which applies the linear transformation to the incoming x tensor\n",
    "x.shape, x.grad_fn  # note that the linear transform was broadcast across the first dimension of x, and that x now has a grad function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82865d42",
   "metadata": {},
   "source": [
    "### Activation layers\n",
    "<mark>Sometimes, the classes don't have any learnable parameters, but it is more convenient to treat them as `nn.Module`s. Activation functions are typical examples:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05bb6441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = nn.ReLU()\n",
    "act.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0767eab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6493, 0.9782, 0.9184, 0.0000, 0.3097, 1.2477],\n",
       "        [0.0000, 0.0000, 0.2852, 0.0000, 0.7568, 0.2353],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7528, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0073, 0.8428, 0.0000],\n",
       "        [0.0000, 0.0641, 0.8788, 0.0000, 0.0717, 0.6831],\n",
       "        [0.0000, 0.2859, 0.6791, 0.0000, 0.3476, 0.8128],\n",
       "        [0.0980, 0.2057, 0.3449, 0.0000, 0.6776, 0.6156],\n",
       "        [0.0000, 0.3588, 0.4100, 0.0000, 0.8765, 1.1306],\n",
       "        [0.0000, 0.7563, 0.2249, 0.0000, 0.6159, 1.4330],\n",
       "        [0.0000, 0.0000, 0.1193, 0.0000, 0.4123, 0.5194]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1348bde",
   "metadata": {},
   "source": [
    "See https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity for the full list of activation functions implemented in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a5bfb",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "Above, we took some data and passed it through a linear layer and then through an activation function. This is a very common action in a neural network. <mark>Sometimes it can be convenient to group together layers and modules into an `nn.Sequential` class, which takes multiple `nn.Module`s and when its `forward` method is called, it will feed the input to the first module and then sequentially feed the output into the next module, and so on, finally returning the output of the last module</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "924d96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_act = nn.Sequential(lin, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36d2570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 7.5461e-01, 2.1361e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 1.2574e-03, 5.5305e-01, 0.0000e+00, 3.2269e-01, 8.7673e-01],\n",
       "        [0.0000e+00, 0.0000e+00, 3.5844e-01, 0.0000e+00, 6.5265e-01, 4.2333e-01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4023e-04, 1.2679e+00, 4.3317e-01],\n",
       "        [0.0000e+00, 8.6462e-01, 0.0000e+00, 0.0000e+00, 1.3742e+00, 1.5711e+00],\n",
       "        [0.0000e+00, 2.3805e-01, 1.1370e+00, 0.0000e+00, 0.0000e+00, 1.0657e+00],\n",
       "        [3.4723e-01, 6.8077e-01, 1.0316e+00, 0.0000e+00, 4.1028e-02, 1.1070e+00],\n",
       "        [3.1998e-01, 1.1306e+00, 8.9459e-01, 0.0000e+00, 2.9035e-01, 1.6655e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 6.2697e-01, 7.6244e-02, 4.2316e-01, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.0623e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_act(torch.randn(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3abb333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.1635,  0.1237, -0.4205,  0.2092],\n",
       "                      [-0.1941,  0.4541, -0.3560,  0.4676],\n",
       "                      [-0.4014, -0.1734, -0.0160,  0.1873],\n",
       "                      [ 0.1532, -0.2695,  0.0614,  0.0183],\n",
       "                      [ 0.3869,  0.1485, -0.0024,  0.0363],\n",
       "                      [-0.1718,  0.4823, -0.1703,  0.4349]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.3623, -0.2044,  0.4976, -0.2128,  0.4963,  0.4201]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_act.state_dict()  # the parameters of the linear layer are still contained in the state_dict o the sequential module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa97b8",
   "metadata": {},
   "source": [
    "### Module lists and dicts\n",
    "Similar to `nn.ParameterList` and `nn.ParameterDict`, <mark>`nn.ModuleList` and `nn.ModuleDict` can be used to contain multiple modules and have them be recognised by the parent `nn.Module` as modules:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc065649",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlist= nn.ModuleList([lin, act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f6c6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(mlist, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01b65ea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlist(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:244\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "mlist(x)  # does not act like a Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5545fb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.3315, 0.0000, 0.7462, 0.6536],\n",
       "        [0.0000, 0.0000, 0.1725, 0.0000, 0.8319, 0.4915],\n",
       "        [0.0000, 0.0707, 0.5480, 0.0000, 0.5823, 0.7847],\n",
       "        [0.0000, 0.0000, 0.1048, 0.0000, 0.8789, 0.3783],\n",
       "        [0.0000, 0.0255, 0.2686, 0.0000, 0.7424, 0.6854],\n",
       "        [0.0000, 0.0000, 0.2530, 0.0000, 0.8254, 0.4819],\n",
       "        [0.0000, 0.0000, 0.1299, 0.0000, 0.8457, 0.5606],\n",
       "        [0.0000, 0.0000, 0.0691, 0.0000, 0.9400, 0.5128],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9867, 0.6592],\n",
       "        [0.0000, 0.0000, 0.3557, 0.0000, 0.7471, 0.5475]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10,4)\n",
    "for m in mlist: x = m(x)  # but can be iterated through\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb684c",
   "metadata": {},
   "source": [
    "## Fowards pass\n",
    "<mark>When building a new module, it is necessary to implement the forwards pass, which defines how the parameters and child modules will affect the incoming tensors. When dealing with high-level PyTorch, the backwards pass will be automatically implemented.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186c1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinAct(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(nin, nout)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd006bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_act = LinAct(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "149505c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7515, 0.9448, 0.4860, 0.0000],\n",
       "        [0.7264, 0.0000, 0.3230, 0.0000],\n",
       "        [0.0744, 0.1151, 0.0000, 0.0000],\n",
       "        [0.9014, 0.0000, 0.5478, 0.0000],\n",
       "        [0.2433, 1.8337, 0.0000, 0.8897]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,2)\n",
    "lin_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85a96bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lin.weight',\n",
       "              tensor([[ 0.6610, -0.1656],\n",
       "                      [-0.5771,  0.5966],\n",
       "                      [ 0.5921,  0.3726],\n",
       "                      [-0.2306,  0.1094]])),\n",
       "             ('lin.bias', tensor([-0.2290,  0.3584,  0.3549,  0.5362]))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_act.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c421157",
   "metadata": {},
   "source": [
    "<mark>In the forward method, nothing is compiled, meaning that conditionals can be used to change what actions are applied</mark>, depending on the data provided at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57a44504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinAct(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(nin, nout)\n",
    "        self.act = nn.ReLU()\n",
    "        self.zero_replacement = nn.Parameter(Tensor([-3]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.act(x)\n",
    "        x[x<=0] = self.zero_replacement  # if any values are less than or equal to 0, replace them with a learnable default value\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37013a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_act = LinAct(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac0adc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2585, -3.0000,  0.4568,  0.7101],\n",
       "        [-3.0000, -3.0000, -3.0000, -3.0000],\n",
       "        [ 1.2667, -3.0000, -3.0000,  0.8093],\n",
       "        [-3.0000, -3.0000,  0.5913, -3.0000],\n",
       "        [-3.0000, -3.0000,  0.1393, -3.0000]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,2)\n",
    "lin_act(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a831e",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "<mark>Suitable initialisation of parameters in a neural network is key to making them trainable quickly (or at all)</mark>. PyTorch provides a default init scheme, but this isn't guaranteed to be suitable for the networks you create; <mark>it should vary according to, at least, the activation function used</mark>. The general <mark>rule-of-thumb is that data sampled from a unit-Gaussian, should retain a unit-Gaussian shape when passed through the DNN. To see why this is important, check out this interactive demo https://www.deeplearning.ai/ai-notes/initialization/index.html.</mark>\n",
    "\n",
    "Two of the most common schemes are:\n",
    "- <mark>Xavier Glorot: weights are sampled from either a uniform distribution between ±sqrt(6/(nin+nout)), or a Gaussian with mean 0 and std sqrt(2/(nin+nout)). This is used for e.g. linear, sigmoid, softmax, and tanh activation functions.</mark>\n",
    "- <mark>Kaiming He: weights are sampled from either a uniform distribution between ±sqrt(3/nin), or a Gaussian with mean 0 and std sqrt(1/nin). This is used for e.g. ReLU, PReLU, Swish, and Mish activation functions.</mark>\n",
    "\n",
    "As part of my LUMIN package, I maintain a list of applicable init schemes here https://github.com/GilesStrong/lumin/blob/master/lumin/nn/models/initialisations.py\n",
    "\n",
    "The <mark>bias of a linear layer can generally be initialised to zeros</mark>\n",
    "\n",
    "A full list of the init schemes in PyTorch is found here https://pytorch.org/docs/stable/nn.init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ff14aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.4279, -0.5738,  0.0695],\n",
       "                      [-0.1682,  0.3995, -0.3439],\n",
       "                      [-0.4387, -0.4449,  0.4781],\n",
       "                      [-0.4473,  0.2167, -0.5135],\n",
       "                      [ 0.5461,  0.3604,  0.0592]])),\n",
       "             ('bias', tensor([-0.2564,  0.5221,  0.1372, -0.2761,  0.2770]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(3,5)\n",
    "lin.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0fc29",
   "metadata": {},
   "source": [
    "<mark>This will set initial values in-place (note the _ at the end of the method to indicate its an in-place operation)</mark>. Since <mark>we expect to feed the output into a ReLU, we need to specify 'relu' for the nonlinearity</mark>. For no discernably good reason at all, there is a <mark>default value of 'leaky_relu' so don't forget to correct this.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17438333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1404, -0.0241,  0.7050],\n",
       "        [-0.0108,  0.2396, -0.4940],\n",
       "        [-0.1710,  0.1022,  0.8184],\n",
       "        [-0.1666,  0.2049, -0.3602],\n",
       "        [ 0.5765, -0.5232,  0.5404]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.kaiming_normal_(lin.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bacb45",
   "metadata": {},
   "source": [
    "Let's zero the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "003f2062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.zeros_(lin.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd1c5ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.1404, -0.0241,  0.7050],\n",
       "                      [-0.0108,  0.2396, -0.4940],\n",
       "                      [-0.1710,  0.1022,  0.8184],\n",
       "                      [-0.1666,  0.2049, -0.3602],\n",
       "                      [ 0.5765, -0.5232,  0.5404]])),\n",
       "             ('bias', tensor([0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4e5ae",
   "metadata": {},
   "source": [
    "<mark>When writing a new module, generally I make sure that the layers are correctly initialised just after they are declared</mark>. However, it is still possible to reinitialise an instantiated module by recursively searching through it for different layers, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54aef91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(model:nn.Module):\n",
    "    r'''Recursively initialise fully-connected ReLU network with Kaiming and zero bias'''\n",
    "    if isinstance(model,nn.Linear):\n",
    "        init.kaiming_normal_(model.weight, nonlinearity='relu')\n",
    "        init.zeros_(model.bias)\n",
    "    for l in model.children(): init_net(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b32d3",
   "metadata": {},
   "source": [
    "However you now lose a bit of control, and it's easy to use the \"wrong\" init scheme depending on the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccc26c",
   "metadata": {},
   "source": [
    "## Saving/loading\n",
    "After training a neural network (or even during), we often want to <mark>save its parameters. This is done via the `state_dict`.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e422c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('zero_replacement', tensor([-3.])),\n",
       "             ('lin.weight',\n",
       "              tensor([[ 0.6140,  0.6540],\n",
       "                      [ 0.5110, -0.1025],\n",
       "                      [-0.2431,  0.2308],\n",
       "                      [ 0.3604,  0.2854]])),\n",
       "             ('lin.bias', tensor([-0.1432, -0.6213,  0.0942,  0.0719]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = lin_act.state_dict()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad7c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state, '03_save.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd49ad0",
   "metadata": {},
   "source": [
    "Now later we may want to reload the saved parameters. Note that <mark>we have only saved the parameters, and not the class or code itself</mark>. So we would need to <mark>re-instantiate the network with random parameters, and then load the saved params into it:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "628026c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('zero_replacement', tensor([-3.])),\n",
       "             ('lin.weight',\n",
       "              tensor([[ 0.0491,  0.6193],\n",
       "                      [-0.7037,  0.1437],\n",
       "                      [ 0.4089,  0.4134],\n",
       "                      [ 0.5533,  0.1788]])),\n",
       "             ('lin.bias', tensor([-0.6660,  0.4402, -0.1380, -0.2449]))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lin_act = LinAct(2,4)\n",
    "new_lin_act.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee093b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = torch.load('03_save.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d40b6627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lin_act.load_state_dict(loaded_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23f4a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('zero_replacement', tensor([-3.])),\n",
       "             ('lin.weight',\n",
       "              tensor([[ 0.6140,  0.6540],\n",
       "                      [ 0.5110, -0.1025],\n",
       "                      [-0.2431,  0.2308],\n",
       "                      [ 0.3604,  0.2854]])),\n",
       "             ('lin.bias', tensor([-0.1432, -0.6213,  0.0942,  0.0719]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lin_act.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a64ed",
   "metadata": {},
   "source": [
    "A more tricky, but flexible and user-convenient export method is the <mark>PyTorch packager https://pytorch.org/docs/stable/package.html. This allows one to export the saved parameters, the code necessary to produce the modules it relates to, and relevant code from any dependencies that users may not want to install.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5ff55",
   "metadata": {},
   "source": [
    "## nn.functional\n",
    "Many operations/layers are presented as classes, but <mark>most are also available as functions; https://pytorch.org/docs/stable/nn.functional.html -- commonly imported as `F`.</mark> Without moving to a full functional-programming approach it is still possible/useful to use these functions in you modules, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9eea3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf59833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinAct(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(nin, nout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = F.relu(x)  # Note rather than have the ReLU be an object of the module, we use the function version\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92607a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.6229, 0.0000, 0.2312, 0.6109],\n",
       "        [0.0000, 0.0000, 0.4761, 0.0000, 0.2108, 0.1984],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5197, 0.9931],\n",
       "        [0.0000, 0.0000, 0.3721, 0.1287, 0.1985, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,2)\n",
    "x = LinAct(2,6)(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58faa7",
   "metadata": {},
   "source": [
    "<mark>A fully functional approach would be something like:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8245505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.init.kaiming_normal_(torch.empty(6,2))\n",
    "b = torch.zeros(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "276d1774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6192, 1.7173, 0.0000, 0.7382, 0.7151, 0.0000],\n",
       "        [0.0000, 2.6393, 0.4481, 7.6317, 0.0000, 0.0000],\n",
       "        [0.9490, 4.2962, 0.0000, 4.8934, 0.7841, 0.0000],\n",
       "        [1.0552, 0.8990, 0.0000, 0.0000, 1.5985, 0.0000]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,2)\n",
    "F.relu(F.linear(x, weight=w, bias=b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7957bf",
   "metadata": {},
   "source": [
    "Or even:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ee308d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6192, 1.7173, 0.0000, 0.7382, 0.7151, 0.0000],\n",
       "        [0.0000, 2.6393, 0.4481, 7.6317, 0.0000, 0.0000],\n",
       "        [0.9490, 4.2962, 0.0000, 4.8934, 0.7841, 0.0000],\n",
       "        [1.0552, 0.8990, 0.0000, 0.0000, 1.5985, 0.0000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp_min(x@w.T+b, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
